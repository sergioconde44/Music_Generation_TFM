{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W2R8Ng8pMWB",
        "outputId": "13bb21ea-e5d1-4ec7-97ad-b69d22edc420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/TFM\n",
            "data\t      file_names.okl  GAN_model     LSTM_Weimar     output.midi\n",
            "example.midi  GAN_MAESTRO     LSTM_ejemplo  model_plot.png\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My\\ Drive/TFM\n",
        "\n",
        "# List files to make sure we're in the expected directory.\n",
        "# Your output will look different, showing your own Drive files here.\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzssEmk3pTfs"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import music21\n",
        "from music21 import converter, instrument, note, chord, stream, pitch#manipular midis\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Bidirectional, LSTM, concatenate, Input\n",
        "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
        "import tensorflow.keras.utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua5nKn5LOIFt"
      },
      "source": [
        "La funcion get_notes(), utiliza la librería music21 para generar el dataset de notas a partir de los archivos midi. De esta forma, tendremos tres variables distintas:\n",
        "\n",
        "- notes: todas las notas que contienen los archivos midi del conjunto de datos\n",
        "- durations: las duraciones de cada una de esas notas.\n",
        "- Offsets: distancia entre la última nota tocada y la nueva.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXvlU1vzUdvb"
      },
      "source": [
        "# Preparación de datos.\n",
        "La funcion get_notes(), utiliza la librería music21 para generar el dataset de notas a partir de los archivos midi. De esta forma, tendremos cuatro variables distintas:\n",
        "\n",
        "- notes: todas las notas que contienen los archivos midi del conjunto de datos\n",
        "- durations: las duraciones de cada una de esas notas.\n",
        "- Offsets: distancia entre la última nota tocada y la nueva.\n",
        "- Chords: Acorde que acompaña a cada una de las notas.\n",
        "\n",
        "Antes de eso, reduciremos los acordes a mayores y menores y ajustaremos la resolución del tiempo a 1/8 de pulso. Después, separaremos los datos en secuencias sucesivas de longitud 100 y utilizaremos el 90% para entrenar y el 10% para test.\n",
        "\n",
        "Como ya tenemos los datos resultantes, nos ahorraremos el proceso y llos cargaremos directamente. Aún así, pondremos las funciones que se utilizaron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqGn9BeQV4-9"
      },
      "outputs": [],
      "source": [
        "# Procesado acordes, los reduciremos a mayores o menores.\n",
        "def proc_chords(chords):\n",
        "  chord_dict = {'A#': 'Bb','B#': 'C','Cb': 'B','C#': 'Db',\n",
        "    'D#': 'Eb','E#': 'F','Fb': 'E','F#': 'Gb','G#': 'Ab',\n",
        "    'A#m': 'Bbm','B#m': 'Cm','Cbm': 'Bm','C#m': 'Dbm','D#m': 'Ebm',\n",
        "    'E#m': 'Fm','Fbm': 'Em','F#m': 'Gbm','G#m': 'Abm', 'NC': 'N'}\n",
        "\n",
        "  for i, ch in enumerate(chords): # eliminamos séptimas etc\n",
        "    ch_type = ''\n",
        "    if len(ch) < 2:\n",
        "      tone = ch[0]\n",
        "    elif ch[1] in ['#', 'b']:\n",
        "      tone = ch[:2]\n",
        "    else:\n",
        "      tone = ch[:1]\n",
        "    if  'm' in ch or '-' in ch:\n",
        "      ch_type = 'm'\n",
        "\n",
        "    chords[i] = tone+ch_type\n",
        "\n",
        "  chords = [chord_dict.get(chord, chord) for chord in chords] # eliminamos la redundancia (Bb = A#)\n",
        "\n",
        "  return chords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9E1upL6hHG5"
      },
      "outputs": [],
      "source": [
        "def round_length(number): # funcion para redondear los tiempos\n",
        "  values_list = [0 , 1/8,  1/4, 3/8, 1/2, 5/8, 6/8,7/8, 1]\n",
        "\n",
        "  entero = int(number)\n",
        "  decim = number - entero\n",
        "  decim = min(values_list, key=lambda x: abs(x - decim))\n",
        "\n",
        "  number = entero + decim\n",
        "  return number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAK0CVnUJwIs"
      },
      "outputs": [],
      "source": [
        "def get_notes():\n",
        "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
        "    notes = []\n",
        "\n",
        "    offsets = []\n",
        "    durations = []\n",
        "    chords = []\n",
        "\n",
        "    filenames = glob.glob('LSTM_Weimar/Weimar_csv/*.csv')\n",
        "    for file in filenames:\n",
        "\n",
        "            mel = pd.read_csv(file)\n",
        "\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            if mel.signature[0] == '\"4/4\"':\n",
        "\n",
        "              notes = numpy.append(notes, mel.pitch.values)\n",
        "              mel['onset'] =  mel.onset.values/mel.beat_duration.median() # el onset y duracion estan en segundos, los pasamos a beats.\n",
        "              mel['onset'] = mel['onset'].apply(round_length)\n",
        "              mel['duration'] =  mel.duration.values/mel.beat_duration.median()\n",
        "              mel['duration'] = mel['duration'].apply(round_length)\n",
        "              mel['duration'] = numpy.where(mel['duration'] > 8, 8, mel['duration']) # las duraciones mayores de un compas las acortamos a un compas\n",
        "              onset = mel.onset.values - mel.onset.shift(1).values\n",
        "              onset[0] = 0\n",
        "              onset = numpy.where(onset > 8, 8, onset)\n",
        "              offsets = numpy.append(offsets, onset)\n",
        "              durations = numpy.append(durations, mel.duration.values)\n",
        "              processed_chords = proc_chords(mel.chord.values)\n",
        "              chords = numpy.append(chords, processed_chords)\n",
        "              print(\"Parsed %s\" % file)\n",
        "            else:\n",
        "              print(file, 'not 4/4')\n",
        "\n",
        "    # for i in range(len(offsets)): #redondeamos los valroes\n",
        "    #         offsets[i] = round_length(offsets[i])\n",
        "\n",
        "    with open('LSTM_Weimar/data_csv_aug/notes_aug', 'wb') as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "\n",
        "    with open('LSTM_Weimar/data_csv_aug/durations_aug', 'wb') as filepath:\n",
        "        pickle.dump(durations, filepath)\n",
        "\n",
        "    with open('LSTM_Weimar/data_csv_aug/offsets_aug', 'wb') as filepath:\n",
        "        pickle.dump(offsets, filepath)\n",
        "\n",
        "    with open('LSTM_Weimar/data_csv_aug/chords_aug', 'wb') as filepath:\n",
        "        pickle.dump(chords, filepath)\n",
        "\n",
        "    return notes, offsets, durations, chords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ-Qdai8XKoJ"
      },
      "outputs": [],
      "source": [
        "# notes, offsets, durations, chords = get_notes()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/notes_aug', \"rb\")\n",
        "notes = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/offsets_aug', \"rb\")\n",
        "offsets = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/durations_aug', \"rb\")\n",
        "durations = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/chords_aug', \"rb\")\n",
        "chords = pickle.load(open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKWBaR5KgKOJ"
      },
      "source": [
        "### Data augmentation\n",
        "Vamos a aumentar el dataset transponiendo las melodias y los acordes 2 semitonos arriba y abajo como en https://arxiv.org/pdf/2008.01307.pdf\n",
        "\n",
        "El dataset resultante ya lo tenemos guardado. Más adelante cargaremos los datos ya preprocesados, así que comentaremos esta parte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLQzoNq2X07H"
      },
      "outputs": [],
      "source": [
        "def transponer_acordes(acordes, semitonos):\n",
        "    notas = numpy.array([\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"Gb\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"])\n",
        "    for i in range(len(acordes)):\n",
        "      for j in range(len(notas)):\n",
        "        if notas[j] in acordes[i]:\n",
        "          notes_trans = numpy.roll(notas, -semitonos)\n",
        "          acordes[i] = acordes[i].replace(notas[j], notes_trans[j])\n",
        "          break\n",
        "\n",
        "    return acordes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV8snR7BcvBe"
      },
      "source": [
        "### Secuencias\n",
        "Con prepare_sequences() separamos los datos en secuencias de longitud, **sequence_lentgh**, y para cada secuencia tenemos una etiqueta, que será el elemento sequence_lentgh + 1. De esta manera nuestro modelo utilizará los primeros *sequence_lentgh* elementos para predecir el siguiente.\n",
        "\n",
        "En concreto, las secuencias de entrada serán vectores normalizados. En cuanto al output, lo representará como una variable categórica con one hot encoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ufcqKdJ078"
      },
      "outputs": [],
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "\t\"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "\tsequence_length = 100\n",
        "\n",
        "\t# get all pitch names\n",
        "\tpitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "\t # create a dictionary to map pitches to integers\n",
        "\tnote_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "\tnetwork_input = []\n",
        "\tnetwork_output = []\n",
        "\n",
        "\t# create input sequences and the corresponding outputs\n",
        "\tfor i in range(0, len(notes) - sequence_length, 1):\n",
        "\t\tsequence_in = notes[i:i + sequence_length]\n",
        "\t\tsequence_out = notes[i + sequence_length]\n",
        "\t\tnetwork_input.append([note_to_int[char] for char in sequence_in])\n",
        "\t\tnetwork_output.append(note_to_int[sequence_out])\n",
        "\n",
        "\tn_patterns = len(network_input)\n",
        "\n",
        "\t# reshape the input into a format compatible with LSTM layers\n",
        "\tnetwork_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "\t# normalize input\n",
        "\tnetwork_input = network_input / float(n_vocab)\n",
        "\n",
        "\tnetwork_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "\treturn (network_input, network_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5U0mcvV0qUo"
      },
      "source": [
        "# Modelo.\n",
        "\n",
        "Comenzamos definiendo el modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkJNjtUJ3wz"
      },
      "outputs": [],
      "source": [
        "def create_network(network_input_notes, n_vocab_notes, network_input_Chords, n_vocab_Chords, network_input_offsets, n_vocab_offsets, network_input_durations, n_vocab_durations):\n",
        "\n",
        "\t# Branch of the network that considers notes\n",
        "\tinputNotesLayer = Input(shape=(network_input_notes.shape[1], network_input_notes.shape[2]))\n",
        "\tinputNotes = LSTM(\n",
        "\t\t256,\n",
        "\t\tinput_shape=(network_input_notes.shape[1], network_input_notes.shape[2]),\n",
        "\t\treturn_sequences=True\n",
        "\t)(inputNotesLayer)\n",
        "\tinputNotes = Dropout(0.2)(inputNotes)\n",
        "\n",
        "\t# Branch of the network that considers chords\n",
        "\tinputChordsLayer = Input(shape=(network_input_Chords.shape[1], network_input_Chords.shape[2]))\n",
        "\tinputChords = LSTM(\n",
        "\t\t256,\n",
        "\t\tinput_shape=(network_input_Chords.shape[1], network_input_Chords.shape[2]),\n",
        "\t\treturn_sequences=True\n",
        "\t)(inputChordsLayer)\n",
        "\tinputChords = Dropout(0.2)(inputChords)\n",
        "\n",
        "\t# Branch of the network that considers note offset\n",
        "\tinputOffsetsLayer = Input(shape=(network_input_offsets.shape[1], network_input_offsets.shape[2]))\n",
        "\tinputOffsets = LSTM(\n",
        "\t\t256,\n",
        "\t\tinput_shape=(network_input_offsets.shape[1], network_input_offsets.shape[2]),\n",
        "\t\treturn_sequences=True\n",
        "\t)(inputOffsetsLayer)\n",
        "\tinputOffsets = Dropout(0.2)(inputOffsets)\n",
        "\n",
        "\t# Branch of the network that considers note duration\n",
        "\tinputDurationsLayer = Input(shape=(network_input_durations.shape[1], network_input_durations.shape[2]))\n",
        "\tinputDurations = LSTM(\n",
        "\t\t256,\n",
        "\t\tinput_shape=(network_input_durations.shape[1], network_input_durations.shape[2]),\n",
        "\t\treturn_sequences=True\n",
        "\t)(inputDurationsLayer)\n",
        "\t#inputDurations = Dropout(0.3)(inputDurations)\n",
        "\tinputDurations = Dropout(0.2)(inputDurations)\n",
        "\n",
        "\t#Concatentate the four input networks together into one branch now\n",
        "\tinputs = concatenate([inputNotes, inputChords, inputOffsets, inputDurations])\n",
        "\n",
        "\t# A cheeky LSTM to consider everything learnt from the four separate branches\n",
        "\tx = LSTM(512, return_sequences=True)(inputs)\n",
        "\tx = Dropout(0.3)(x)\n",
        "\tx = LSTM(512)(x)\n",
        "\tx = BatchNorm()(x)\n",
        "\tx = Dropout(0.3)(x)\n",
        "\tx = Dense(256, activation='relu')(x)\n",
        "\n",
        "\t#Time to split into four branches again...\n",
        "\n",
        "\t# Branch of the network that classifies the note\n",
        "\toutputNotes = Dense(128, activation='relu')(x)\n",
        "\toutputNotes = BatchNorm()(outputNotes)\n",
        "\toutputNotes = Dropout(0.3)(outputNotes)\n",
        "\toutputNotes = Dense(n_vocab_notes, activation='softmax', name=\"Note\")(outputNotes)\n",
        "\n",
        "  # Branch of the network that classifies the note\n",
        "\toutputChords = Dense(128, activation='relu')(x)\n",
        "\toutputChords = BatchNorm()(outputChords)\n",
        "\toutputChords = Dropout(0.3)(outputChords)\n",
        "\toutputChords = Dense(n_vocab_Chords, activation='softmax', name=\"Chord\")(outputChords)\n",
        "\n",
        "\t# Branch of the network that classifies the note offset\n",
        "\toutputOffsets = Dense(128, activation='relu')(x)\n",
        "\toutputOffsets = BatchNorm()(outputOffsets)\n",
        "\toutputOffsets = Dropout(0.3)(outputOffsets)\n",
        "\toutputOffsets = Dense(n_vocab_offsets, activation='softmax', name=\"Offset\")(outputOffsets)\n",
        "\n",
        "\t# Branch of the network that classifies the note duration\n",
        "\toutputDurations = Dense(128, activation='relu')(x)\n",
        "\toutputDurations = BatchNorm()(outputDurations)\n",
        "\toutputDurations = Dropout(0.3)(outputDurations)\n",
        "\toutputDurations = Dense(n_vocab_durations, activation='softmax', name=\"Duration\")(outputDurations)\n",
        "\n",
        "\t# Tell Keras what our inputs and outputs are\n",
        "\tmodel = Model(inputs=[inputNotesLayer, inputChordsLayer,inputOffsetsLayer, inputDurationsLayer], outputs=[outputNotes, outputChords, outputOffsets, outputDurations])\n",
        "\n",
        "\t#Adam seems to be faster than RMSProp and learns better too\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# Useful to try RMSProp though\n",
        "\n",
        "\t# LOAD WEIGHTS HERE IF YOU WANT TO CONTINUE TRAINING!\n",
        "\tmodel.load_weights('LSTM_ejemplo/modelo_csv_aug/weights-improvement-68-4.1149-bigger.hdf5')\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVT_RQ7e1y8w"
      },
      "source": [
        "# Entrenamiento.\n",
        "\n",
        "Defininimos la funciónde entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smvzvlCsJuYJ"
      },
      "outputs": [],
      "source": [
        "def train_network(notes, chords, offsets, durations):\n",
        "\n",
        "    n_vocab_notes = len(set(notes))\n",
        "\n",
        "    n_vocab_chords = len(set(chords))\n",
        "\n",
        "    n_vocab_offsets = len(set(offsets))\n",
        "\n",
        "    n_vocab_durations = len(set(durations))\n",
        "\n",
        "    path = 'LSTM_ejemplo/data_csv_aug/'\n",
        "    # Cargamos las variables de entrenamiento\n",
        "    train_input_notes = numpy.load(path + 'train_input_notes.npy')\n",
        "    train_output_notes = numpy.load(path + 'train_output_notes.npy')\n",
        "    train_input_durations = numpy.load(path + 'train_input_durations.npy')\n",
        "    train_output_durations = numpy.load(path + 'train_output_durations.npy')\n",
        "    train_input_offsets = numpy.load(path + 'train_input_offsets.npy')\n",
        "    train_output_offsets = numpy.load(path + 'train_output_offsets.npy')\n",
        "    train_input_chords = numpy.load(path + 'train_input_chords.npy')\n",
        "    train_output_chords = numpy.load(path + 'train_output_chords.npy')\n",
        "\n",
        "    # Cargamos los datos de prueba\n",
        "    test_input_notes = numpy.load(path + 'test_input_notes.npy')\n",
        "    test_output_notes = numpy.load(path + 'test_output_notes.npy')\n",
        "    test_input_durations = numpy.load(path + 'test_input_durations.npy')\n",
        "    test_output_durations = numpy.load(path + 'test_output_durations.npy')\n",
        "    test_input_offsets = numpy.load(path + 'test_input_offsets.npy')\n",
        "    test_output_offsets = numpy.load(path + 'test_output_offsets.npy')\n",
        "    test_input_chords = numpy.load(path + 'test_input_chords.npy')\n",
        "    test_output_chords = numpy.load(path + 'test_output_chords.npy')\n",
        "\n",
        "    train_input = [train_input_notes, train_input_chords, train_input_offsets, train_input_durations]\n",
        "    train_output = [train_output_notes, train_output_chords, train_output_offsets, train_output_durations]\n",
        "    test_input = [test_input_notes, test_input_chords, test_input_offsets, test_input_durations]\n",
        "    test_output = [test_output_notes, test_output_chords, test_output_offsets, test_output_durations]\n",
        "\n",
        "    model = create_network(train_input_notes, n_vocab_notes, train_input_chords, n_vocab_chords, train_input_offsets, n_vocab_offsets, train_input_durations, n_vocab_durations)\n",
        "    model.summary()\n",
        "    train(model, train_input, train_output, test_input, test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqpfVK9vAOXM"
      },
      "outputs": [],
      "source": [
        "class SaveHistoryCallback(Callback):\n",
        "    def __init__(self, file_path):\n",
        "        super(SaveHistoryCallback, self).__init__()\n",
        "        self.file_path = file_path\n",
        "        self.history_list = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Append the current epoch's training and validation metrics to the history list\n",
        "        self.history_list.append(logs)\n",
        "\n",
        "        # Save the history list to the file using pickle\n",
        "        with open(self.file_path, 'wb') as file:\n",
        "            pickle.dump(self.history_list, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otvT9EJsJ9Vf"
      },
      "outputs": [],
      "source": [
        "def train(model, train_input, train_output, test_input, test_output):\n",
        "\t\"\"\" train the neural network \"\"\"\n",
        "\tfilepath = \"LSTM_Weimar/modelo_csv_aug/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "\tcheckpoint = ModelCheckpoint(\n",
        "\t\tfilepath,\n",
        "\t\tmonitor='loss',\n",
        "\t\tverbose=0,\n",
        "\t\tsave_best_only=True,\n",
        "\t\tmode='min'\n",
        "\t)\n",
        "\n",
        "\tcustom_callback = SaveHistoryCallback(file_path='LSTM_ejemplo/modelo_csv_aug/history.pkl')\n",
        "\n",
        "\tcallbacks_list = [checkpoint, custom_callback]\n",
        "\n",
        "\n",
        "\tmodel.fit(train_input, train_output, epochs=300, initial_epoch = 68, batch_size=64, callbacks=callbacks_list, verbose=1, validation_data = (test_input, test_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnbfMubskkzo"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\ttrain_network(notes, chords, offsets, durations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLDjc4hoNSg1"
      },
      "source": [
        "# Generación\n",
        "\n",
        "Para la generación de nuevas melodías se utiliza un input formado por una secuencia de notas, acordes, duraciones y offsets, cada una de distinta secuencia original. De esta forma la secuencia resultante es diferente a cualquiera ya existente. Utilizando esta nueva secuencia se generaran las nuevas notas.\n",
        "\n",
        "Comenzamos cargando los datos y definiendo las funciones que utilizamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqRCNMjyQF2K"
      },
      "outputs": [],
      "source": [
        "\"\"\" Generate a piano midi file \"\"\"\n",
        "#load the notes used to train the model\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/notes_aug', \"rb\")\n",
        "notes = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/offsets_aug', \"rb\")\n",
        "offsets = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/durations_aug', \"rb\")\n",
        "durations = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "open_file = open('LSTM_ejemplo/data_csv_aug/chords_aug', \"rb\")\n",
        "chords = pickle.load(open_file)\n",
        "open_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIpv0Ik54kz1"
      },
      "outputs": [],
      "source": [
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "\t\"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "\t# map between notes and integers and back\n",
        "\tnote_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "\tsequence_length = 100\n",
        "\tnetwork_input = []\n",
        "\toutput = []\n",
        "\tfor i in range(0, len(notes) - sequence_length, 1):\n",
        "\t\tsequence_in = notes[i:i + sequence_length]\n",
        "\t\tsequence_out = notes[i + sequence_length]\n",
        "\t\tnetwork_input.append([note_to_int[char] for char in sequence_in])\n",
        "\t\toutput.append(note_to_int[sequence_out])\n",
        "\n",
        "\tn_patterns = len(network_input)\n",
        "\n",
        "\t# reshape the input into a format compatible with LSTM layers\n",
        "\tnormalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "\t# normalize input\n",
        "\tnormalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "\treturn (network_input, normalized_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GagZfHPQ42Ld"
      },
      "source": [
        "Definimos las funciones para transformar los vectores a archivos midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "253p-KrH4311"
      },
      "outputs": [],
      "source": [
        "def get_chord_notes(ch):\n",
        "\tif 'm' in ch:\n",
        "\t\tch = ch.replace('m', '', 1)\n",
        "\t\tmid_pitch = pitch.Pitch(ch).midi\n",
        "\t\tch = [mid_pitch, mid_pitch + 3, mid_pitch + 7]\n",
        "\n",
        "\telse:\n",
        "\t\tmid_pitch = pitch.Pitch(ch).midi\n",
        "\t\tch = [mid_pitch, mid_pitch + 4, mid_pitch + 7]\n",
        "\n",
        "\treturn chord.Chord(ch)\n",
        "\n",
        "\n",
        "def create_midi(pred_output):\n",
        "\t\"\"\" convert the output from the prediction to notes and create a midi file\n",
        "\t\tfrom the notes \"\"\"\n",
        "\toffset = 0\n",
        "\toutput_notes = []\n",
        "\toutput_chords = []\n",
        "\tch = 'N'\n",
        "   # todas las columnas se convierten a string, los transformmos\n",
        "\tnotes = pred_output[0].astype(float).astype(int).tolist()\n",
        "\tchords = pred_output[1].tolist()\n",
        "\tdurations = pred_output[2].astype(float).tolist()\n",
        "\toffsets = pred_output[3].astype(float).tolist()\n",
        "\n",
        "\n",
        "\tprint(\"---\")\n",
        "\tprint(\"Creating Midi File...\")\n",
        "\n",
        "\t# create note and chord objects based on the values generated by the model\n",
        "\tx = 0 # this is the counter\n",
        "\n",
        "\tfor pattern in notes:\n",
        "\t\toffset += offsets[x]\n",
        "\t\tnew_note = note.Note(pattern)\n",
        "\t\tnew_note.offset = offset\n",
        "\t\tnew_note.duration.quarterLength = durations[x]\n",
        "\t\toutput_notes.append(new_note)\n",
        "\n",
        "\t\t# for chords\n",
        "\t\tif (chords[x] != ch) & (chords[x] != 'N'): # cuando cambia el acorde\n",
        "\t\t\tif ch != 'N': #añadimos el viejo acorde tras el cambio. La duraacion se extendera hasta el cambio de acorde.\n",
        "\t\t\t\tnew_chord.offset = chord_offset\n",
        "\t\t\t\tnew_chord.duration.quarterLength = offset - chord_offset\n",
        "\t\t\t\toutput_chords.append(new_chord)\n",
        "\t\t\t\t# print(new_chord.offset)\n",
        "\n",
        "\t\t\tnew_chord = get_chord_notes(chords[x]) #añadimos lo que sera el futuro nuevo acorde\n",
        "\t\t\tchord_offset = offset\n",
        "\t\t\tch = chords[x]\n",
        "\n",
        "\t\tif x == len(notes)-1:\n",
        "\t\t\tif ch != 'N': #añadimos el viejo acorde tras el cambio. La duraacion se extendera hasta el cambio de acorde.\n",
        "\t\t\t\tnew_chord.offset = chord_offset\n",
        "\t\t\t\tnew_chord.duration.quarterLength = offset - chord_offset\n",
        "\t\t\t\toutput_chords.append(new_chord)\n",
        "\n",
        "\t\t# increase offset each iteration so that notes do not stack\n",
        "\t\tx = x+1\n",
        "\n",
        "\tstream_notes = stream.Stream(output_notes)\n",
        "\n",
        "\tstream_chords = stream.Stream(output_chords)\n",
        "\tstream_notes.insert(instrument.Flute())\n",
        "\tstream_chords.insert(instrument.Piano())\n",
        "\n",
        "\tmidi_stream = stream.Stream()\n",
        "\tmidi_stream.insert(music21.tempo.MetronomeMark(number = 90))\n",
        "\tmidi_stream.insert(0,stream_notes)\n",
        "\tmidi_stream.insert(0,stream_chords)\n",
        "\n",
        "\tmidi_stream.write('midi', fp='LSTM_ejemplo/ejemplo_Weimar_CSV_4.mid')\n",
        "\n",
        "\tprint(\"Midi created!\")\n",
        "\treturn midi_stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3wmoeQignUn"
      },
      "outputs": [],
      "source": [
        "def generate(notes, chords, offsets, durations):\n",
        "\n",
        "\tnotenames = sorted(set(item for item in notes))\n",
        "\tn_vocab_notes = len(set(notes))\n",
        "\tnetwork_input_notes, normalized_input_notes = prepare_sequences(notes, notenames, n_vocab_notes)\n",
        "\n",
        "\toffsetnames = sorted(set(item for item in offsets))\n",
        "\tn_vocab_offsets = len(set(offsets))\n",
        "\tnetwork_input_offsets, normalized_input_offsets = prepare_sequences(offsets, offsetnames, n_vocab_offsets)\n",
        "\n",
        "\tdurationames = sorted(set(item for item in durations))\n",
        "\tn_vocab_durations = len(set(durations))\n",
        "\tnetwork_input_durations, normalized_input_durations = prepare_sequences(durations, durationames, n_vocab_durations)\n",
        "\n",
        "\tchordnames = sorted(set(item for item in chords))\n",
        "\tn_vocab_chords = len(set(chords))\n",
        "\tnetwork_input_chords, normalized_input_chords = prepare_sequences(chords, chordnames, n_vocab_chords)\n",
        "\n",
        "\tmodel = create_network(normalized_input_notes, n_vocab_notes, normalized_input_chords, n_vocab_chords, normalized_input_offsets, n_vocab_offsets, normalized_input_durations, n_vocab_durations)\n",
        "\n",
        "\tprediction_output = generate_notes(model, network_input_notes, network_input_chords, network_input_offsets, network_input_durations, notenames, chordnames, offsetnames, durationames, n_vocab_notes, n_vocab_chords, n_vocab_offsets, n_vocab_durations)\n",
        "\tcreate_midi(prediction_output)\n",
        "\n",
        "\n",
        "\n",
        "def generate_notes(model, network_input_notes, network_input_chords, network_input_offsets, network_input_durations, notenames, chordnames, offsetnames, durationames, n_vocab_notes, n_vocab_chords, n_vocab_offsets, n_vocab_durations):\n",
        "\t\"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "\t# pick a random sequence from the input as a starting point for the prediction\n",
        "\tstart = numpy.random.randint(0, len(network_input_notes)-1)\n",
        "\tstart2 = numpy.random.randint(0, len(network_input_offsets)-1)\n",
        "\tstart3 = numpy.random.randint(0, len(network_input_durations)-1)\n",
        "\t# start4 = numpy.random.randint(0, len(network_input_chords)-1)\n",
        "\n",
        "\tint_to_note = dict((number, note) for number, note in enumerate(notenames))\n",
        "\tnote_to_int = {v: k for k, v in int_to_note.items()}\n",
        "\tint_to_offset = dict((number, note) for number, note in enumerate(offsetnames))\n",
        "\toffset_to_int = {v: k for k, v in int_to_offset.items()}\n",
        "\tint_to_duration = dict((number, note) for number, note in enumerate(durationames))\n",
        "\tduration_to_int = {v: k for k, v in int_to_duration.items()}\n",
        "\tint_to_chord = dict((number, note) for number, note in enumerate(chordnames))\n",
        "\tchord_to_int = {v: k for k, v in int_to_chord.items()}\n",
        "\n",
        "\tpattern = network_input_notes[start]\n",
        "\tpattern2 = network_input_offsets[start2]\n",
        "\tpattern3 = network_input_durations[start3]\n",
        "\tpattern4 = network_input_chords[start]\n",
        "\n",
        "\tprediction_output = []\n",
        "\n",
        "\t# generate notes or chords\n",
        "\tfor note_index in range(100):\n",
        "\t\tnote_prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\t\tpredictedNote = note_prediction_input[-1][-1][-1]\n",
        "\t\tnote_prediction_input = note_prediction_input / float(n_vocab_notes)\n",
        "\n",
        "\t\toffset_prediction_input = numpy.reshape(pattern2, (1, len(pattern2), 1))\n",
        "\t\toffset_prediction_input = offset_prediction_input / float(n_vocab_offsets)\n",
        "\n",
        "\t\tduration_prediction_input = numpy.reshape(pattern3, (1, len(pattern3), 1))\n",
        "\t\tduration_prediction_input = duration_prediction_input / float(n_vocab_durations)\n",
        "\n",
        "\t\tchord_prediction_input = numpy.reshape(pattern4, (1, len(pattern4), 1))\n",
        "\t\tchord_prediction_input = chord_prediction_input / float(n_vocab_chords)\n",
        "\n",
        "\t\tprediction = model.predict([note_prediction_input, chord_prediction_input, offset_prediction_input, duration_prediction_input], verbose=0)\n",
        "\n",
        "\t\tindex = numpy.argmax(prediction[0])\n",
        "\t\tresult = int_to_note[index]\n",
        "\n",
        "\t\tchord = numpy.argmax(prediction[1])\n",
        "\t\tchord_result = int_to_chord[chord]\n",
        "\n",
        "\t\toffset = numpy.argmax(prediction[2])\n",
        "\t\toffset_result = int_to_offset[offset]\n",
        "\n",
        "\t\tduration = numpy.argmax(prediction[3])\n",
        "\t\tduration_result = int_to_duration[duration]\n",
        "\n",
        "\n",
        "\t\tprint(\"Next note: \" + str(int_to_note[predictedNote]) + \" - Chord: \" + str(int_to_chord[chord]) + \" - Duration: \" + str(int_to_duration[duration]) + \" - Offset: \" + str(int_to_offset[offset]))\n",
        "\n",
        "\n",
        "\t\tprediction_output.append([result, chord_result, offset_result, duration_result])\n",
        "\n",
        "\t\tpattern.append(index)\n",
        "\t\tpattern2.append(offset)\n",
        "\t\tpattern3.append(duration)\n",
        "\t\tpattern4.append(chord)\n",
        "\t\tpattern = pattern[1:len(pattern)]\n",
        "\t\tpattern2 = pattern2[1:len(pattern2)]\n",
        "\t\tpattern3 = pattern3[1:len(pattern3)]\n",
        "\t\tpattern4 = pattern4[1:len(pattern4)]\n",
        "\n",
        "\treturn numpy.transpose(prediction_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeqO8ieQ7_6s"
      },
      "source": [
        "### ejecutar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miLmUB_751vw"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\tgenerate(notes, chords, offsets, durations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}